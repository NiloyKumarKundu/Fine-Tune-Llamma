{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niloy/miniconda3/envs/finetune/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Continuous , short @-@ arc , high pressure xenon arc lamps have a color temperature closely approximating noon sunlight and are used in solar simulators . That is , the chromaticity of these lamps closely approximates a heated black body radiator that has a temperature close to that observed from the Sun . After they were first introduced during the 1940s , these lamps began replacing the shorter @-@ lived carbon arc lamps in movie projectors . They are employed in typical 35mm , IMAX and the new digital projectors film projection systems , automotive HID headlights , high @-@ end \" tactical \" flashlights and other specialized uses . These arc lamps are an excellent source of short wavelength ultraviolet radiation and they have intense emissions in the near infrared , which is used in some night vision systems . \n",
      "\n",
      " Field Marshal Antonio Jos√© de Sucre is portrayed as an intimate friend of the General . The historical Antonio Jos√© de Sucre , the Field Marshal of Ayacucho , had been the most trusted general of Sim√≥n Bol√≠var . Garc√≠a M√°rquez describes him as \" intelligent , methodical , shy , and superstitious \" . The Field Marshal is married to and has a daughter with Do√±a Mariana Carcel√©n . In the first chapter of the novel , the General asks Sucre to succeed him as President of the Republic , but he rejects the idea . One of the reasons Sucre gives is that he wishes only to live his life for his family . Also at the beginning of the novel , Sucre 's death is foreshadowed . Sucre tells the General that he plans on celebrating the Feast of Saint Anthony in Quito with his family . When the General hears that Sucre has been assassinated in Berruecos on his way back to Quito , he vomits blood . \n",
      "\n",
      " Norman Gary Finkelstein ( born December 8 , 1953 ) is an American political scientist , activist , professor , and author . His primary fields of research are the Israeli ‚Äì Palestinian conflict and the politics of the Holocaust , an interest motivated by the experiences of his parents who were Jewish Holocaust survivors . He is a graduate of Binghamton University and received his Ph.D in political science at Princeton University . He has held faculty positions at Brooklyn College , Rutgers University , Hunter College , New York University , and DePaul University where he was an assistant professor from 2001 to 2007 . \n",
      "\n",
      " Galveston has several state @-@ funded charter schools not affiliated with local school districts , including kindergarten through 8th grade Ambassadors Preparatory Academy and pre @-@ kindergarten through 8th Grade Odyssey Academy . In addition KIPP : the Knowledge Is Power Program opened KIPP Coastal Village in Galveston under the auspices of GISD . \n",
      "\n",
      " Walpole 's works have not been completely neglected in recent years . The Herries stories have seldom been out of print , and in 2014 WorldCat listed a dozen recent reissues of Walpole 's works , including The Wooden Horse , The Dark Forest , The Secret City , Jeremy , and The Cathedral . In 2011 the BBC broadcast a reappraisal of Walpole , The Walpole Chronicle , presented by Eric Robson . In 2013 a new stage version of Rogue Herries was presented by the Theatre by the Lake company in Walpole 's adopted home of Keswick . The BBC speculated that this could mark a revival in interest in his works . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access the train dataset\n",
    "train_dataset = dataset['train']\n",
    "    \n",
    "# Shuffle the train dataset and display 5 random rows\n",
    "shuffled_train = train_dataset.shuffle(seed=42)\n",
    "\n",
    "# Show random rows\n",
    "for i in range(5):\n",
    "    print(shuffled_train[i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niloy/miniconda3/envs/finetune/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load pre-trained GPT-2 tokenizer and model\n",
    "model_name = \"gpt2\"  # You can change this to another model if needed (like GPT-Neo)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation before fine-tuning\n",
    "def generate_text_before(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fine-tuning:\n",
      "Once upon a time a boy was born, he was called a boy. He was called a boy because he was born with a boy's head. He was called a boy because he was born with a boy's head. He was called a boy\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time a boy\"\n",
    "print(\"Before fine-tuning:\")\n",
    "print(generate_text_before(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Tokenize the text\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Group the text into chunks and create labels\n",
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n",
    "              for k, t in concatenated_examples.items()}\n",
    "    \n",
    "    # Add labels which are the same as input_ids\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niloy/miniconda3/envs/finetune/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Training arguments for fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=7,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),  # Enables mixed precision if GPU supports it\n",
    "    save_steps=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niloy/miniconda3/envs/finetune/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32669' max='32669' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32669/32669 49:32, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.461400</td>\n",
       "      <td>3.417959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.270200</td>\n",
       "      <td>3.417857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.081400</td>\n",
       "      <td>3.442904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.966900</td>\n",
       "      <td>3.471915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.842300</td>\n",
       "      <td>3.499979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.650200</td>\n",
       "      <td>3.530347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.611000</td>\n",
       "      <td>3.552998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32669, training_loss=2.941487056740702, metrics={'train_runtime': 2973.0793, 'train_samples_per_second': 43.951, 'train_steps_per_second': 10.988, 'total_flos': 8535692132352000.0, 'train_loss': 2.941487056740702, 'epoch': 7.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7: Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2-finetuned/tokenizer_config.json',\n",
       " './gpt2-finetuned/special_tokens_map.json',\n",
       " './gpt2-finetuned/vocab.json',\n",
       " './gpt2-finetuned/merges.txt',\n",
       " './gpt2-finetuned/added_tokens.json',\n",
       " './gpt2-finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./gpt2-finetuned\")\n",
    "tokenizer.save_pretrained(\"./gpt2-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned GPT-2 model and tokenizer\n",
    "fine_tuned_model_path = \"./gpt2-finetuned\"  # Path to your fine-tuned model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation after fine-tuning\n",
    "def generate_text_after(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, num_return_sequences=1, attention_mask=inputs[\"attention_mask\"], eos_token_id=None)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning:\n",
      "Once upon a time a boy named John, who had been sent to fetch the boy, was abducted by the Templars. John was taken to the Templars'hideout in the mountains, where he was tortured and executed. \n",
      " = =\n"
     ]
    }
   ],
   "source": [
    "print(\"After fine-tuning:\")\n",
    "prompt = \"Once upon a time a boy\"\n",
    "print(generate_text_after(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
